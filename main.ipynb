{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-7v_e1HEx5g"
      },
      "source": [
        "# Detecting Decentralized Solar Panel Assemblies in Post-War Syria\n",
        "\n",
        "This project introduces a novel workflow for identifying solar panel assemblies in the challenging, post-war urban landscapes of Syria. Decentralized solar electricity production has become common in Syria due to unreliability and inconsistent delivery from the national electric grid. Our primary goal was to develop a deep learning model capable of not only detecting panels but also performing instance segmentation to estimate their area and keypoint regression to determine each panel's cardinal direction. Both of these outputs are important for accurately estimating energy production from residential solar, which is vital for grid integration and reconstruction efforts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGfKUAg0Ex5i"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Existing models for solar panel identification have been mainly trained on data from Western countries with very different conditions--large, uniform solar farms, buildings with slanted, uncluttered roofs--from the Syrian context. This environment includes panels installed on flat, crowded rooftops in non-standardized configurations. The nature of and difficulties in acquiring our data are key distinguishing features of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3E9JT03Ex5i"
      },
      "source": [
        "### Data Acquisition Challenges\n",
        "\n",
        "Our data was acquired through the invaluable efforts of local, on-the-ground aerial photographers. However, this process was fraught with difficulties. The challenges included:\n",
        "\n",
        "* **Limited Equipment**: Access was restricted to consumer-focused drones, which are equipped with only consumer-level cameras, and can only cover limited areas at lower altitudes.\n",
        "* **Security Clearances**: Obtaining the necessary permits to fly drones can be exceptionally difficult in volatile regions like Syria.\n",
        "* **Environmental Conditions**: Windy conditions pose a significant challenge for smaller, consumer-grade drones, affecting image stability and quality, especially at higher altitudes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwTZJGyLEx5i"
      },
      "source": [
        "### Hurdles in Syrian Urban Landscapes\n",
        "\n",
        "Another major hurdle is the unique nature of urban spaces in Syria, which feature dense, cluttered rooftops. This environment creates numerous challenges for accurate solar panel detection. Below are some examples of these difficult factors:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BsVCEz2Ex5j"
      },
      "source": [
        "**a. Dense and Obstructed Rooftops**\n",
        "\n",
        "Urban density leads to many obstructions and features like building windows, balconies, cars (especially sunroofs), and decorative tiled patterns that easily be mistaken for solar panels.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;margin: 25px 0;\">\n",
        "<img src=\"main_ipynb_images/roof_pattern.png\" width=\"300\" alt=\"A grid structure on a roof\"><img src=\"main_ipynb_images/roof_pattern2.png\" width=\"300\" alt=\"A grid structure on a roof\"><img src=\"main_ipynb_images/confusing_patterns.png\" width=\"300\" alt=\"Tiles in a square pattern can confuse the model. \">\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JNPRf5bEx5j"
      },
      "source": [
        "**b. Solar Water Heaters**\n",
        "\n",
        "Solar water heaters, which are irrelevant to our goal of estimating electric output, are common features on rooftops in Syria and appear visually similar to the photovoltaic (PV) solar panels we aim to detect.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;margin: 25px 0;\">\n",
        "<img src=\"main_ipynb_images/water_heater2.png\" width=\"400\" alt=\"Solar water heaters are widespread and look like PV panels.\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1KgjXoTEx5j"
      },
      "source": [
        "**c. Limited and Varied Data**\n",
        "\n",
        "The limited nature of our data introduces further complications:\n",
        "* **Shadows**: Depending on the time of day and their interplay with the geometry of buildings, shadows can be mistaken for panels.\n",
        "* **Camera Angle**: Images are not always taken from a straight-down (orthographic) perspective, leading to distortion and lower image quality, especially away from the center.\n",
        "\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;margin: 25px 0;\">\n",
        "<img src=\"main_ipynb_images/dense_rooftops_at_an_angle.png\" width=\"300\" alt=\"Low resolution and dense rooftops make it difficult to identify solar panels.\"><img src=\"main_ipynb_images/distorted_low_quality.png\" width=\"300\" alt=\"At a sharp angle and dark shadows can be confused for panels.\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1217j--GEx5j"
      },
      "source": [
        "### Methodological Choice: Corners over Angles\n",
        "\n",
        "Our model identifies the two bottom corners of a solar panel assembly rather than predict its angle directly for two primary reasons:\n",
        "\n",
        "1.  **Data Limitation**: A direct angle prediction would require strictly orthographic (straight-down) images. By focusing on corners, we can still utilize images taken from an angle along with every part of images taken straight-down regardless of perspective distortion. For the images that are indeed orthographic, the angle can be derived by taking the direction perpendicular to the line connecting the two corners.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;margin: 25px 0;\">\n",
        "<img src=\"main_ipynb_images/calculating_angle.png\" width=\"400\" alt=\"By finding the bottom corners, we automatically can calculate the direction in straight-down images.\">\n",
        "</div>\n",
        "\n",
        "\n",
        "2.  **Loss Function**: The loss function for a direct angle prediction would be non-differentiable due to its cyclic nature (e.g., the difference between 359° and 1° is small, but a simple subtraction yields a large value). A regression task to find corner coordinates, where we can simply use MSE, avoids this issue.\n",
        "\n",
        "<div style=\"display: flex; justify-content: center;margin: 25px 0;\">\n",
        "<img src=\"main_ipynb_images/loss_function_y=0.png\" width=\"400\" alt=\"Graph of an 'L1' cyclic loss function between two angles, where one of the angles is fixed to zero. \">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzFU3CkYEx5j"
      },
      "source": [
        "## Labeling the Data\n",
        "\n",
        "1.  Our local photographers took 10 drone images from diverse neighborhoods varying by socioeconomic level, damage to property, and type (residential, commercial) in the city of Homs, Syria.\n",
        "\n",
        "2.  **Patch Sampling**: We sampled 1544 image patches of size 224x224 from 9 of our source images that we reserve for training and validation. The sampling process involved random rotations (applied *before* labeling to avoid the difficulty of transforming our annotations by rotation) and ensured minimal overlap between patches (the center of one patch cannot appear in another).\n",
        "\n",
        "3.  **Test Set**: To prevent information leakage from the training and validation sets, we kept one source image from a separate geographical location aside. From this image, we sampled 109 patches to form our final test set.\n",
        "\n",
        "4.  **Custom Annotation Tools**: We developed custom tools to facilitate the hand-labeling of our images with all necessary annotations: bounding boxes, segmentation masks, and the two bottom corners for each solar panel assembly. A separate tool was created to display these annotations for verification. Using these, we meticulously labeled and verified our data to produce annotated COCO JSON files.\n",
        "\n",
        "5.  **Active Learning Workflow**: To manage the extensive labeling effort, we employed an active learning strategy. We began by randomly selecting and labeling 300 image patches. We then used this initial training set to identify the 200 most impactful (i.e. uncertain) examples from the unlabeled pool to label next. Our acquisition function was the average classification entropy for the bounding box predictions in a given patch. To address cases of model overconfidence, we employed a multiple inference strategy varying our model hyperparameters. This process was repeated twice more, each time adding 100 more images. This iterative approach allowed us to build a robust training set of 700 fully hand-labeled image patches, leaving 844 patches from the original training/validation pool unlabeled.\n",
        "\n",
        "\n",
        "    <div style=\"display: flex; justify-content: center;margin: 25px 0;\">\n",
        "    <img src=\"main_ipynb_images/active_learning_entropy_graph.png\" width=\"600\" alt=\"Overview of our models\">\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaikgZBrEx5j"
      },
      "source": [
        "## Our Models\n",
        "\n",
        "1.  **Three-Stage Architecture**: Our model employs a three-stage pipeline:\n",
        "    * **Stage 1 (Object Detection)**: A **Faster R-CNN** model identifies and produces a list of bounding boxes for the solar panels in an image.\n",
        "    * **Stage 2 (Segmentation)**: A version of **SlimSAM** takes the image and a single bounding box at a time to generate a precise segmentation mask for each detected solar panel.\n",
        "    * **Stage 3 (Corner Prediction)**: A custom model predicts the coordinates of the two bottom corners. This model takes the image, bounding box, and mask for a single panel as input. It utilizes a **ResNet50 with FPN backbone shared with the Faster R-CNN** to extract image features, a small CNN to process the mask, and simple MLPs to process the bounding box coordinates and combine the information from all three processed inputs.\n",
        "\n",
        "\n",
        "    <div style=\"display: flex; justify-content: center;margin: 25px 0;\">\n",
        "    <img src=\"main_ipynb_images/model_architecture.png\" width=\"600\" alt=\"Overview of our models\">\n",
        "    </div>\n",
        "  \n",
        "\n",
        "2.  **Training Routine**: We developed a 5-phase training routine to train the Faster R-CNN and the corner prediction model jointly, leveraging their shared backbone. The SlimSAM model was not fine-tuned as it performed exceptionally well out-of-the-box. The 5 phases are as follows:\n",
        "    * **Phase 0**: Freeze the backbone and train only the head of the corner model on a subset of the data. This is necessary because the corner model's head is initialized randomly, unlike the pre-trained head of the Faster R-CNN.\n",
        "    * **Phase 1**: Train the heads of both the bounding box and corner models on the same subset of data.\n",
        "    * **Phase 2**: Freeze the heads and fine-tune the shared backbone using losses from both tasks simultaneously on the remainder of the data.\n",
        "    * **Phase 3**: Re-train the heads alone on the entire labeled dataset.\n",
        "    * **Phase 4**: Fine-tune the complete models on the entire dataset.\n",
        "    \n",
        "    The hyperparameters for this routine were tuned using Optuna. More details can be found in the `multistage_multitask_training.ipynb` notebook.\n",
        "    \n",
        "3.  **Inference and Deployment**: We have implemented functions for loading the trained models and running the full inference pipeline. Additionally, a small web application and Python server code (`server.py` and `inference_app.html`) were created to allow users to interact with the saved models through a web interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f-RCOAjEx5k"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "To evaluate the performance of our segmentation model, we used the **Jaccard Index (Intersection over Union - IoU)**, **Accuracy**, and **F1-Score**. The code for this evaluation is contained in `solareval.py`. The following code cell will run the evaluation on our test set and display the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIivHoAiEx5k",
        "outputId": "59db92de-ae0b-4a4f-8fa1-1c8fb2e15d1b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Bounding box model loaded from /content/drive/MyDrive/Erdos Institute - Solar Panels Project/models/2025_08_07_02_13_model_bbox.pth\n",
            "SAM model loaded from Zigeng/SlimSAM-uniform-77\n",
            "Loaded 109 images from /content/drive/MyDrive/Erdos Institute - Solar Panels Project/data/test_images with annotations in /content/drive/MyDrive/Erdos Institute - Solar Panels Project/data/test_set_annotations.json.\n",
            "Evaluating model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 109/109 [00:20<00:00,  5.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Results:\n",
            "{'avg_iou': 0.6323783993721008, 'avg_accuracy': 0.9903662204742432, 'avg_f1_score': 0.7747938632965088}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet torchmetrics\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "\n",
        "BASE_PROJECT_PATH = '/content/drive/MyDrive/Erdos Institute - Solar Panels Project'\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append(os.path.join(BASE_PROJECT_PATH, 'modules'))\n",
        "from solarutils import *\n",
        "\n",
        "from solareval import calculate_metrics\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "MODEL_PATH = os.path.join(BASE_PROJECT_PATH, 'models', \"2025_08_07_02_13_model_bbox.pth\")\n",
        "bbox_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load models.\n",
        "\n",
        "bbox_model = load_bbox_model(DEVICE, path=MODEL_PATH)\n",
        "#baseline_model = load_bbox_model(DEVICE)\n",
        "sam_model, sam_processor = load_sam_model(\"Zigeng/SlimSAM-uniform-77\", DEVICE)\n",
        "\n",
        "# Define paths\n",
        "TEST_IMAGE_DIR = os.path.join(BASE_PROJECT_PATH, 'data', 'test_images')\n",
        "TEST_JSON = os.path.join(BASE_PROJECT_PATH, 'data', 'test_set_annotations.json')\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = calculate_metrics(bbox_model, sam_model, sam_processor, bbox_transform, TEST_IMAGE_DIR, TEST_JSON, DEVICE, bbox_threshold= 0.5)\n",
        "print(\"Evaluation Results:\")\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBHZ2HZOx3YE",
        "outputId": "a73084be-5fbb-45f7-aa0c-cb1ea038114b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using provided backbone for feature extraction.\n",
            "Corner predictor model loaded from /content/drive/MyDrive/Erdos Institute - Solar Panels Project/models/2025_08_11_16_20_model_corner.pth\n"
          ]
        }
      ],
      "source": [
        "CORNER_MODEL_PATH = os.path.join(BASE_PROJECT_PATH, 'models', \"2025_08_11_16_20_model_corner.pth\")\n",
        "corner_model = load_corner_model(CORNER_MODEL_PATH, bbox_model.backbone, DEVICE, strategy = 'basic')\n",
        "corner_img_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "corner_mask_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "data = SinglePanelDataset(TEST_JSON, TEST_IMAGE_DIR, image_transform = corner_img_transform, mask_transform = corner_mask_transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mWw3Oz6rzApF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "MSE = 0\n",
        "for image, target in data:\n",
        "  image = image.to(DEVICE)\n",
        "  target['box_xywh'] = target['box_xywh'].to(DEVICE)\n",
        "  target['mask'] = target['mask'].to(DEVICE)\n",
        "  MSE+=(target['keypoints'].to(DEVICE)-corner_model(image.unsqueeze(0), target['box_xywh'].unsqueeze(0), target['mask'].unsqueeze(0)))**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT4fHfNI0KNr",
        "outputId": "b5e50a11-069a-421f-91a5-0c3f19867557",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1811.8848, device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(MSE.squeeze(0)/len(data)*(224**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dNv0nXCOaHU"
      },
      "source": [
        "## Results: IoU = 0.63, Accuracy = 0.99, F1-score = 0.77.\n",
        "For the corner model: MSE between the target and predicted 4-dimensional vectors is 1811."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For comparison, here are the values of these metrics for different models and datasets from the SolarFormer paper, along with a baseline where we use a non-fine-tuned pre-trained Faster R-CNN model with the same SlimSAM workflow: \n",
        "\n",
        "<div style=\"display: flex; justify-content: center;margin: 25px 0;\">\n",
        "    <img src=\"main_ipynb_images/metrics.png\" width=\"600\" alt=\"Overview of our models\">\n",
        "    </div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOvIqQaYEx5k"
      },
      "source": [
        "## Further Improvements\n",
        "\n",
        "While this project has yielded promising results, there are several avenues for future improvement, especially for the corner model:\n",
        "\n",
        "* **Data Procurement**: Acquiring more high-quality, orthographic drone imagery would be the most direct way to improve model accuracy and reduce the impact of issues like perspective distortion.\n",
        "* **Context-Aware Corner Prediction**: An interesting direction would be to have the model first use the general image context to determine the angle of the sun (e.g., from shadows), and then combine this information with the bounding box and mask to produce more accurate corner predictions.\n",
        "* **Alternative Active Learning Strategies**: Our project uses a standard uncertainty-based acquisition function (classification entropy). We would like to explore other uncertainty-based approaches like Monte Carlo dropout or other families of acquisition functions like clustering methods.\n",
        "* **Model Architecture**: Exploring transformer-based models, such as Vision Transformers (ViT), could potentially lead to performance gains.\n",
        "* **Model Efficiency**: For wide-scale deployment in a resource-limited environment like Syria, developing smaller, more efficient models that maintain high accuracy would be a critical next step, especially for nationwide deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E-fu4CnPOdq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
